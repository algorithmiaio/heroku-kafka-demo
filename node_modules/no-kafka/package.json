{
  "_args": [
    [
      {
        "raw": "no-kafka@git://github.com/crcastle/kafka.git#3.0",
        "scope": null,
        "escapedName": "no-kafka",
        "name": "no-kafka",
        "rawSpec": "git://github.com/crcastle/kafka.git#3.0",
        "spec": "git://github.com/crcastle/kafka.git#3.0",
        "type": "hosted",
        "hosted": {
          "type": "github",
          "ssh": "git@github.com:crcastle/kafka.git#3.0",
          "sshUrl": "git+ssh://git@github.com/crcastle/kafka.git#3.0",
          "httpsUrl": "git+https://github.com/crcastle/kafka.git#3.0",
          "gitUrl": "git://github.com/crcastle/kafka.git#3.0",
          "shortcut": "github:crcastle/kafka#3.0",
          "directUrl": "https://raw.githubusercontent.com/crcastle/kafka/3.0/package.json"
        }
      },
      "/Users/ahmadalnaimi/Desktop/heroku-kafka/sentiment"
    ]
  ],
  "_from": "git://github.com/crcastle/kafka.git#3.0",
  "_id": "no-kafka@2.5.6",
  "_inCache": true,
  "_location": "/no-kafka",
  "_phantomChildren": {},
  "_requested": {
    "raw": "no-kafka@git://github.com/crcastle/kafka.git#3.0",
    "scope": null,
    "escapedName": "no-kafka",
    "name": "no-kafka",
    "rawSpec": "git://github.com/crcastle/kafka.git#3.0",
    "spec": "git://github.com/crcastle/kafka.git#3.0",
    "type": "hosted",
    "hosted": {
      "type": "github",
      "ssh": "git@github.com:crcastle/kafka.git#3.0",
      "sshUrl": "git+ssh://git@github.com/crcastle/kafka.git#3.0",
      "httpsUrl": "git+https://github.com/crcastle/kafka.git#3.0",
      "gitUrl": "git://github.com/crcastle/kafka.git#3.0",
      "shortcut": "github:crcastle/kafka#3.0",
      "directUrl": "https://raw.githubusercontent.com/crcastle/kafka/3.0/package.json"
    }
  },
  "_requiredBy": [
    "/"
  ],
  "_resolved": "git://github.com/crcastle/kafka.git#8b6be55ffb14e3a3d09c32eca42e00a32f146f33",
  "_shasum": "6821da42a10f1ab91039a16c7498370b31681b49",
  "_shrinkwrap": null,
  "_spec": "no-kafka@git://github.com/crcastle/kafka.git#3.0",
  "_where": "/Users/ahmadalnaimi/Desktop/heroku-kafka/sentiment",
  "author": {
    "name": "Oleksiy Krivoshey",
    "email": "oleksiyk@gmail.com"
  },
  "bugs": {
    "url": "https://github.com/oleksiyk/kafka/issues"
  },
  "dependencies": {
    "bin-protocol": "^3.0.2",
    "bluebird": "^3.3.3",
    "buffer-crc32": "^0.2.5",
    "hashring": "~=3.2.0",
    "lodash": "^4.5.0",
    "murmur-hash-js": "^1.0.0",
    "nice-simple-logger": "^1.0.1",
    "snappy": "^5.0.0",
    "wrr-pool": "^1.0.3"
  },
  "description": "Apache Kafka 0.9 client for Node.JS",
  "devDependencies": {
    "chai": "^3.5.0",
    "chai-as-promised": "^5.2.0",
    "eslint": "^2.2.0",
    "eslint-config-magictoolbox": "^0.0.2",
    "istanbul": "^0.4.2",
    "mocha": "^2.4.5",
    "sinon": "^1.4.0",
    "sinon-chai": "^2.8.0"
  },
  "gitHead": "8b6be55ffb14e3a3d09c32eca42e00a32f146f33",
  "homepage": "http://github.com/oleksiyk/kafka",
  "keywords": [
    "kafka"
  ],
  "license": "MIT",
  "main": "./lib/index.js",
  "name": "no-kafka",
  "optionalDependencies": {},
  "readme": "[![Build Status][badge-travis]][travis]\n[![Test Coverage][badge-coverage]][coverage]\n[![david Dependencies][badge-david-deps]][david-deps]\n[![david Dev Dependencies][badge-david-dev-deps]][david-dev-deps]\n[![license][badge-license]][license]\n\n# no-kafka\n\n__no-kafka__ is [Apache Kafka](https://kafka.apache.org) 0.9 client for Node.js with [new unified consumer API](#groupconsumer-new-unified-consumer-api) support.\n\nSupports sync and async Gzip and Snappy compression, producer batching and controllable retries, offers few predefined group assignment strategies and producer partitioner option.\n\nAll methods will return a [promise](https://github.com/petkaantonov/bluebird)\n\n* [Using](#using)\n* [Producer](#producer)\n  * [Keyed Messages](#keyed-messages)\n  * [Batching (grouping) produce requests](#batching-grouping-produce-requests)\n  * [Custom Partitioner](#custom-partitioner)\n  * [Producer options](#producer-options)\n* [Simple Consumer](#simpleconsumer)\n  * [Simple Consumer options](#simpleconsumer-options)\n* [Group Consumer](#groupconsumer-new-unified-consumer-api)\n  * [Assignment strategies](#assignment-strategies)\n  * [Group Consumer options](#groupconsumer-options)\n* [Group Admin](#groupadmin-consumer-groups-api)\n* [Compression](#compression)\n* [Connection](#connection)\n  * [SSL](#ssl)\n* [Logging](#logging)\n* [Topic Creation](#topic-creation)\n* [License](#license)\n\n## Using\n\n* [download and install Kafka](https://kafka.apache.org/documentation.html#quickstart)\n* create your test topic:\n\n```shell\nkafka-topics.sh --zookeeper 127.0.0.1:2181 --create --topic kafka-test-topic --partitions 3 --replication-factor 1\n```\n\n* install __no-kafka__\n\n```shell\nnpm install no-kafka\n```\n\n\n## Producer\n\nExample:\n\n```javascript\nvar Kafka = require('no-kafka');\nvar producer = new Kafka.Producer();\n\nreturn producer.init().then(function(){\n  return producer.send({\n      topic: 'kafka-test-topic',\n      partition: 0,\n      message: {\n          value: 'Hello!'\n      }\n  });\n})\n.then(function (result) {\n  /*\n  [ { topic: 'kafka-test-topic', partition: 0, offset: 353 } ]\n  */\n});\n```\n\nSend and retry if failed within 100ms delay:\n\n```javascript\nreturn producer.send(messages, {\n  retries: {\n    attempts: 2,\n    delay: {\n      min: 100,\n      max: 300\n    }\n  }\n});\n```\n\n### Batching (grouping) produce requests\n\nAccumulate messages into single batch until their total size is >= 1024 bytes or 100ms timeout expires (overwrite Producer constructor options):\n\n```javascript\nproducer.send(messages, {\n  batch: {\n    size: 1024,\n    maxWait: 100\n  }\n});\nproducer.send(messages, {\n  batch: {\n    size: 1024,\n    maxWait: 100\n  }\n});\n```\n\nPlease note, that if you pass different options to the `send()` method then these messages will be grouped into separate batches:\n\n```javascript\n// will be sent in batch 1\nproducer.send(messages, {\n  batch: {\n    size: 1024,\n    maxWait: 100\n  },\n  codec: Kafka.COMPRESSION_GZIP\n});\n// will be sent in batch 2\nproducer.send(messages, {\n  batch: {\n    size: 1024,\n    maxWait: 100\n  },\n  codec: Kafka.COMPRESSION_SNAPPY\n});\n```\n\n### Keyed Messages\n\nSend a message with the key:\n\n```javascript\nproducer.send({\n    topic: 'kafka-test-topic',\n    partition: 0,\n    message: {\n        key: 'some-key'\n        value: 'Hello!'\n    }\n});\n```\n\n### Custom Partitioner\n\nExample: override the default partitioner with a custom partitioner that only uses a portion of the key.\n\n```javascript\nvar util  = require('util');\nvar Kafka = require('no-kafka');\n\nvar Producer           = Kafka.Producer;\nvar DefaultPartitioner = Kafka.DefaultPartitioner;\n\nfunction MyPartitioner() {\n    DefaultPartitioner.apply(this, arguments);\n}\n\nutil.inherits(MyPartitioner, DefaultPartitioner);\n\nMyPartitioner.prototype.getKey = function getKey(message) {\n    return message.key.split('-')[0];\n};\n\nvar producer = new Producer({\n    partitioner : new MyPartitioner()\n});\n\nreturn producer.init().then(function(){\n  return producer.send({\n      topic: 'kafka-test-topic',\n      message: {\n          key   : 'namespace-key',\n          value : 'Hello!'\n      }\n  });\n});\n```\n\n### Producer options:\n* `requiredAcks` - require acknoledgments for produce request. If it is 0 the server will not send any response.  If it is 1 (default), the server will wait the data is written to the local log before sending a response. If it is -1 the server will block until the message is committed by all in sync replicas before sending a response. For any number > 1 the server will block waiting for this number of acknowledgements to occur (but the server will never wait for more acknowledgements than there are in-sync replicas).\n* `timeout` - timeout in ms for produce request\n* `clientId` - ID of this client, defaults to 'no-kafka-client'\n* `connectionString` - comma delimited list of initial brokers list, defaults to '127.0.0.1:9092'\n* `reconnectionDelay` - controls optionally progressive delay between reconnection attempts in case of network error:\n  * `min` - minimum delay, used as increment value for next attempts, defaults to 1000ms\n  * `max` - maximum delay value, defaults to 1000ms\n* `partitioner` - Class instance used to determine topic partition for message. If message already specifies a partition, the partitioner won't be used. The partitioner must inherit from [`Kafka.DefaultPartitioner`](lib/assignment/partitioners/default.js). The `partition` method receives 3 arguments: the topic name, an array with topic partitions, and the message (useful to partition by key, etc.). `partition` can be sync or async (return a Promise).\n* `retries` - controls number of attempts at delay between them when produce request fails\n  * `attempts` - number of total attempts to send the message, defaults to 3\n  * `delay` - controls delay between retries, the delay is progressive and incrememented with each attempt with `min` value steps up to but not exceeding `max` value\n    * `min` - minimum delay, used as increment value for next attempts, defaults to 1000ms\n    * `max` - maximum delay value, defaults to 3000ms\n* `codec` - compression codec, one of Kafka.COMPRESSION_NONE, Kafka.COMPRESSION_SNAPPY, Kafka.COMPRESSION_GZIP\n* `batch` - control batching (grouping) of requests\n  * `size` - group messages together into single batch until their total size exceeds this value, defaults to 16384 bytes. Set to 0 to disable batching.\n  * `maxWait` - send grouped messages after this amount of milliseconds expire even if their total size doesn't exceed `batch.size` yet, defaults to 10ms. Set to 0 to disable batching.\n* `asyncCompression` - boolean, use asynchronouse compression instead of synchronous, defaults to `false`\n\n## SimpleConsumer\n\nManually specify topic, partition and offset when subscribing. Suitable for simple use cases.\n\nExample:\n\n```javascript\nvar consumer = new Kafka.SimpleConsumer();\n\n// data handler function can return a Promise\nvar dataHandler = function (messageSet, topic, partition) {\n    messageSet.forEach(function (m) {\n        console.log(topic, partition, m.offset, m.message.value.toString('utf8'));\n    });\n};\n\nreturn consumer.init().then(function () {\n    // Subscribe partitons 0 and 1 in a topic:\n    return consumer.subscribe('kafka-test-topic', [0, 1], dataHandler);\n});\n```\n\nSubscribe (or change subscription) to specific offset and limit maximum received MessageSet size:\n\n```javascript\nconsumer.subscribe('kafka-test-topic', 0, {offset: 20, maxBytes: 30}, dataHandler)\n```\n\nSubscribe to latest or earliest offsets in the topic/parition:\n\n```javascript\nconsumer.subscribe('kafka-test-topic', 0, {time: Kafka.LATEST_OFFSET}, dataHandler)\nconsumer.subscribe('kafka-test-topic', 0, {time: Kafka.EARLIEST_OFFSET}, dataHandler)\n```\n\nSubscribe to all partitions in a topic:\n\n```javascript\nconsumer.subscribe('kafka-test-topic', dataHandler)\n```\n\nCommit offset(s) (V0, Kafka saves these commits to Zookeeper)\n\n```javascript\nconsumer.commitOffset([\n  {\n      topic: 'kafka-test-topic',\n      partition: 0,\n      offset: 1\n  },\n  {\n      topic: 'kafka-test-topic',\n      partition: 1,\n      offset: 2\n  }\n])\n```\n\nFetch commited offset(s)\n\n```javascript\nconsumer.fetchOffset([\n  {\n      topic: 'kafka-test-topic',\n      partition: 0\n  },\n  {\n      topic: 'kafka-test-topic',\n      partition: 1\n  }\n]).then(function (result) {\n/*\n[ { topic: 'kafka-test-topic',\n    partition: 1,\n    offset: 2,\n    metadata: null,\n    error: null },\n  { topic: 'kafka-test-topic',\n    partition: 0,\n    offset: 1,\n    metadata: null,\n    error: null } ]\n*/\n});\n```\n\n### SimpleConsumer options\n* `groupId` - group ID for comitting and fetching offsets. Defaults to 'no-kafka-group-v0'\n* `maxWaitTime` - maximum amount of time in milliseconds to block waiting if insufficient data is available at the time the fetch request is issued, defaults to 100ms\n* `idleTimeout` - timeout between fetch calls, defaults to 1000ms\n* `minBytes` - minimum number of bytes to wait from Kafka before returning the fetch call, defaults to 1 byte\n* `maxBytes` - maximum size of messages in a fetch response, defaults to 1MB\n* `clientId` - ID of this client, defaults to 'no-kafka-client'\n* `connectionString` - comma delimited list of initial brokers list, defaults to '127.0.0.1:9092'\n* `reconnectionDelay` - controls optionally progressive delay between reconnection attempts in case of network error:\n  * `min` - minimum delay, used as increment value for next attempts, defaults to 1000ms\n  * `max` - maximum delay value, defaults to 1000ms\n* `recoveryOffset` - recovery position (time) which will used to recover subscription in case of OffsetOutOfRange error, defaults to Kafka.LATEST_OFFSET\n* `asyncCompression` - boolean, use asynchronouse decompression instead of synchronous, defaults to `false`\n* `handlerConcurrency` - specify concurrency level for the consumer handler function, defaults to 10\n\n## GroupConsumer (new unified consumer API)\n\nSpecify an assignment strategy (or use __no-kafka__ built-in consistent or round robin assignment strategy) and subscribe by specifying only topics. Elected group leader will automatically assign partitions between all group members.\n\nExample:\n\n```javascript\nvar Promise = require('bluebird');\nvar consumer = new Kafka.GroupConsumer();\n\nvar dataHandler = function (messageSet, topic, partition) {\n    return Promise.each(messageSet, function (m){\n        console.log(topic, partition, m.offset, m.message.value.toString('utf8'));\n        // commit offset\n        return consumer.commitOffset({topic: topic, partition: partition, offset: m.offset, metadata: 'optional'});\n    });\n};\n\nvar strategies = [{\n    subscriptions: ['kafka-test-topic'],\n    handler: dataHandler\n}];\n\nconsumer.init(strategies); // all done, now wait for messages in dataHandler\n```\n\n### Assignment strategies\n\n__no-kafka__ provides three built-in strategies:\n* `Kafka.WeightedRoundRobinAssignmentStrategy` weighted round robin assignment (based on [wrr-pool](https://github.com/oleksiyk/wrr-pool)).\n* `Kafka.ConsistentAssignmentStrategy` which is based on a consistent [hash ring](https://github.com/3rd-Eden/node-hashring) and so provides consistent assignment across consumers in a group based on supplied `metadata.id` and `metadata.weight` options.\n* `Kafka.DefaultAssignmentStrategy` simple round robin assignment strategy (default).\n\nUsing `Kafka.WeightedRoundRobinAssignmentStrategy`:\n\n```javascript\nvar strategies = {\n    strategy: 'TestStrategy',\n    subscriptions: ['kafka-test-topic'],\n    metadata: {\n        weight: 4\n    },\n    strategy: new Kafka.WeightedRoundRobinAssignmentStrategy(),\n    handler: dataHandler\n};\n// consumer.init(strategies)....\n```\n\nUsing `Kafka.ConsistentAssignmentStrategy`:\n\n```javascript\nvar strategies = {\n    subscriptions: ['kafka-test-topic'],\n    metadata: {\n        id: process.argv[2] || 'consumer_1',\n        weight: 50\n    },\n    strategy: new Kafka.ConsistentAssignmentStrategy(),\n    handler: dataHandler\n};\n// consumer.init(strategies)....\n```\nNote that each consumer in a group should have its own and consistent metadata.id.\n\nYou can also write your own assignment strategy by inheriting from Kafka.DefaultAssignmentStrategy and overwriting `assignment` method.\n\n### GroupConsumer options\n\n* `groupId` - group ID for comitting and fetching offsets. Defaults to 'no-kafka-group-v0.9'\n* `maxWaitTime` - maximum amount of time in milliseconds to block waiting if insufficient data is available at the time the fetch request is issued, defaults to 100ms\n* `idleTimeout` - timeout between fetch calls, defaults to 1000ms\n* `minBytes` - minimum number of bytes to wait from Kafka before returning the fetch call, defaults to 1 byte\n* `maxBytes` - maximum size of messages in a fetch response\n* `clientId` - ID of this client, defaults to 'no-kafka-client'\n* `connectionString` - comma delimited list of initial brokers list, defaults to '127.0.0.1:9092'\n* `reconnectionDelay` - controls optionally progressive delay between reconnection attempts in case of network error:\n  * `min` - minimum delay, used as increment value for next attempts, defaults to 1000ms\n  * `max` - maximum delay value, defaults to 1000ms\n* `sessionTimeout` - session timeout in ms, min 6000, max 30000, defaults to `15000`\n* `heartbeatTimeout` - delay between heartbeat requests in ms, defaults to `1000`\n* `retentionTime` - offset retention time in ms, defaults to 1 day (24 * 3600 * 1000)\n* `startingOffset` - starting position (time) when there is no commited offset, defaults to `Kafka.LATEST_OFFSET`\n* `recoveryOffset` - recovery position (time) which will used to recover subscription in case of OffsetOutOfRange error, defaults to Kafka.LATEST_OFFSET\n* `asyncCompression` - boolean, use asynchronouse decompression instead of synchronous, defaults to `false`\n* `handlerConcurrency` - specify concurrency level for the consumer handler function, defaults to 10\n\n## GroupAdmin (consumer groups API)\n\nOffes two methods:\n\n* `listGroups` - list existing consumer groups\n* `describeGroup` - describe existing group by its id\n\nExample:\n\n```javascript\nvar admin = new Kafka.GroupAdmin();\n\nreturn admin.init().then(function(){\n    return admin.listGroups().then(function(groups){\n        // [ { groupId: 'no-kafka-admin-test-group', protocolType: 'consumer' } ]\n        return admin.describeGroup('no-kafka-admin-test-group').then(function(group){\n            /*\n            { error: null,\n              groupId: 'no-kafka-admin-test-group',\n              state: 'Stable',\n              protocolType: 'consumer',\n              protocol: 'DefaultAssignmentStrategy',\n              members:\n               [ { memberId: 'group-consumer-82646843-b4b8-4e91-94c9-b4708c8b05e8',\n                   clientId: 'group-consumer',\n                   clientHost: '/192.168.1.4',\n                   version: 0,\n                   subscriptions: [ 'kafka-test-topic'],\n                   metadata: <Buffer 63 6f 6e 73 75 6d 65 72 2d 6d 65 74 61 64 61 74 61>,\n                   memberAssignment:\n                    { _blength: 44,\n                      version: 0,\n                      partitionAssignment:\n                       [ { topic: 'kafka-test-topic',\n                           partitions: [ 0, 1, 2 ] },\n                          ],\n                      metadata: null } },\n                  ] }\n             */\n        })\n    });\n});\n```\n\n## Compression\n\n__no-kafka__ supports both SNAPPY and Gzip compression.\n\nEnable compression in Producer:\n\n```javascript\nvar Kafka = require('no-kafka');\n\nvar producer = new Kafka.Producer({\n    clientId: 'producer',\n    codec: Kafka.COMPRESSION_SNAPPY // Kafka.COMPRESSION_NONE, Kafka.COMPRESSION_SNAPPY, Kafka.COMPRESSION_GZIP\n});\n```\n\nAlternatively just send some messages with specified compression codec (overwrites codec set in contructor):\n\n```javascript\nreturn producer.send({\n    topic: 'kafka-test-topic',\n    partition: 0,\n    message: { value: 'p00' }\n}, { codec: Kafka.COMPRESSION_SNAPPY })\n```\n\nBy default __no-kafka__ will use asynchronous compression and decompression.\nDisable async compression/decompression (and use sync) with `asyncCompression` option (synchronous Gzip is not availble in node < 0.11):\n\nProducer:\n\n```javascript\nvar producer = new Kafka.Producer({\n    clientId: 'producer',\n    asyncCompression: false, // use sync compression/decompression\n    codec: Kafka.COMPRESSION_SNAPPY\n});\n```\n\nConsumer:\n\n```javascript\nvar consumer = new Kafka.SimpleConsumer({\n    idleTimeout: 100,\n    clientId: 'simple-consumer',\n    asyncCompression: true\n});\n```\n\n## Connection\n\n__no-kafka__ will connect to the hosts specified in `connectionString` constructor option unless it is omitted. In this case it will use KAFKA_URL environment variable or fallback to default `kafka://127.0.0.1:9092`. For better availability always specify several initial brokers: `10.0.1.1:9092,10.0.1.2:9092,10.0.1.3:9092`. The `kafka://` prefix is optional.\n\nAll network errors are handled by the library: producer will retry sending failed messages for configured amount of times, simple consumer and group consumer will try to reconnect to failed host, update metadata as needed as so on.\n\n### SSL\nTo connect to Kafka with [SSL endpoint enabled](http://kafka.apache.org/090/documentation.html#security_ssl) specify SSL certificate and key file options:\n\n```javascript\nvar producer = new Kafka.Producer({\n  connectionString: 'kafka://127.0.0.1:9093', // should match `listeners` SSL option in Kafka config\n  ssl: {\n    certFile: '/path/to/client.crt',\n    keyFile: '/path/to/client.key'\n  }\n});\n```\n\nOther Node.js SSL options are available such as `rejectUnauthorized`, `secureProtocol`, `ciphers`, etc. See Node.js `tls.createServer` method documentation for more details.\n\nIt is also possible to use `KAFKA_CLIENT_CERT` and `KAFKA_CLIENT_CERT_KEY` environment variables to specify SSL certificate and key locations:\n\n```bash\nKAFKA_URL=kafka://127.0.0.1:9093 KAFKA_CLIENT_CERT=./test/ssl/client.crt KAFKA_CLIENT_CERT_KEY=./test/ssl/client.key node producer.js\n```\n\n### Reconnection delay\nIn case of network error which prevents further operations __no-kafka__ will try to reconnect to Kafka brokers in a endless loop with the optionally progressive delay which can be configured with `reconnectionDelay` option.\n\n## Logging\n\nYou can differentiate messages from several instances of producer/consumer by providing unique `clientId` in options:\n\n```javascript\nvar consumer1 = new Kafka.GroupConsumer({\n    clientId: 'group-consumer-1'\n});\nvar consumer2 = new Kafka.GroupConsumer({\n    clientId: 'group-consumer-2'\n});\n```\n=>\n\n```\n2016-01-12T07:41:57.884Z INFO group-consumer-1 ....\n2016-01-12T07:41:57.884Z INFO group-consumer-2 ....\n```\n\nChange the logging level:\n\n```javascript\nvar consumer = new Kafka.GroupConsumer({\n    clientId: 'group-consumer',\n    logger: {\n        logLevel: 1 // 0 - nothing, 1 - just errors, 2 - +warnings, 3 - +info, 4 - +debug, 5 - +trace\n    }\n});\n```\n\nSend log messages to Logstash server(s) via UDP:\n\n```javascript\nvar consumer = new Kafka.GroupConsumer({\n    clientId: 'group-consumer',\n    logger: {\n        logstash: {\n            enabled: true,\n            connectionString: '10.0.1.1:9999,10.0.1.2:9999',\n            app: 'myApp-kafka-consumer'\n        }\n    }\n});\n```\n\nYou can overwrite the function that outputs messages to stdout/stderr:\n\n```javascript\nvar consumer = new Kafka.GroupConsumer({\n    clientId: 'group-consumer',\n    logger: {\n        logFunction: console.log\n    }\n});\n```\n\n## Topic Creation\n\nThere is no Kafka API call to create a topic. Kafka supports auto creating of topics when their metadata is first requested (`auto.create.topic` option) but the topic is created with all default parameters, which is useless. There is no way to be notified when the topic has been created, so the library will need to ping the server with some interval. There is also no way to be notified of any error for this operation. For this reason, having no guarantees, __no-kafka__ won't provide topic creation method until there will be a specific Kafka API call to create/manage topics.\n\n## License: [MIT](https://github.com/oleksiyk/kafka/blob/master/LICENSE)\n\n[badge-license]: https://img.shields.io/badge/License-MIT-green.svg\n[license]: https://github.com/oleksiyk/kafka/blob/master/LICENSE\n[badge-travis]: https://api.travis-ci.org/oleksiyk/kafka.svg?branch=master\n[travis]: https://travis-ci.org/oleksiyk/kafka\n[badge-coverage]: https://codeclimate.com/github/oleksiyk/kafka/badges/coverage.svg\n[coverage]: https://codeclimate.com/github/oleksiyk/kafka/coverage\n[badge-david-deps]: https://david-dm.org/oleksiyk/kafka.svg\n[david-deps]: https://david-dm.org/oleksiyk/kafka\n[badge-david-dev-deps]: https://david-dm.org/oleksiyk/kafka/dev-status.svg\n[david-dev-deps]: https://david-dm.org/oleksiyk/kafka#info=devDependencies\n[badge-bithound-code]: https://www.bithound.io/github/oleksiyk/kafka/badges/code.svg\n[bithound-code]: https://www.bithound.io/github/oleksiyk/kafka\n[badge-bithound-overall]: https://www.bithound.io/github/oleksiyk/kafka/badges/score.svg\n[bithound-overall]: https://www.bithound.io/github/oleksiyk/kafka\n[badge-bithound-deps]: https://www.bithound.io/github/oleksiyk/kafka/badges/dependencies.svg\n[bithound-deps]: https://www.bithound.io/github/oleksiyk/kafka/master/dependencies/npm\n[badge-bithound-dev-deps]: https://www.bithound.io/github/oleksiyk/kafka/badges/devDependencies.svg\n[bithound-dev-deps]: https://www.bithound.io/github/oleksiyk/kafka/master/dependencies/npm\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/oleksiyk/kafka.git"
  },
  "scripts": {
    "test": "make"
  },
  "version": "2.5.6"
}
